{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832",
   "metadata": {},
   "source": [
    "## Assignment 3: $k$ Nearest Neighbor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9212c0",
   "metadata": {},
   "source": [
    "**Q1.**\n",
    "1. What is the difference between regression and classification?\n",
    "2. What is a confusion table? What does it help us understand about a model's performance?\n",
    "3. What does the SSE quantify about a particular model?\n",
    "4. What are overfitting and underfitting? \n",
    "5. Why does splitting the data into training and testing sets, and choosing $k$ by evaluating accuracy or SSE on the test set, improve model performance?\n",
    "6. With classification, we can report a class label as a prediction or a probability distribution over class labels. Please explain the strengths and weaknesses of each approach."
   ]
  },
  {
   "cell_type": "raw",
   "id": "873db877-9cce-41fe-8a42-dae19e370d3a",
   "metadata": {},
   "source": [
    "1. Regression is used when we want to predict a numerical value, like a price or temperature. Classification is used when we want to predict a category or label, such as \"spam\" versus \"not spam\".\n",
    "\n",
    "2. A confusion table shows how often a model's predictions match the true labels and where it makes mistakes. It helps us see which classes the model predicts well and which ones it struggles with.\n",
    "\n",
    "3. The sum of squared errors (SSE) measures how far the model's predictions are from the true values overall. Smaller SSE means the model's predictions are closer to the actual data.\n",
    "\n",
    "4. Overfitting happens when a model learns the training data too closely and does not generalize well to new data. Underfitting happens when a model is too simple and misses important patterns in the data.\n",
    "\n",
    "5. Splitting data into training and testing sets lets us evaluate how well a model works on new, unseen data. Choosing parameters like \n",
    "k based on test performance helps avoid models that only work well on the training data.\n",
    "\n",
    "6. Predicting a class label is simple and easy to interpret, but it does not show how confident the model is. Predicting probabilities gives more information about uncertainty, but it can be harder to interpret and explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194455fa",
   "metadata": {},
   "source": [
    "**Q2.** This question is a case study for $k$ nearest neighbor regression, using the `USA_cars_datasets.csv` data.\n",
    "\n",
    "The target variable `y` is `price` and the features are `year` and `mileage`.\n",
    "\n",
    "1. Load the `./data/USA_cars_datasets.csv`. Keep the following variables and drop the rest: `price`, `year`, `mileage`. Are there any `NA`'s to handle? Look at the head and dimensions of the data.\n",
    "2. Maxmin normalize `year` and `mileage`.\n",
    "3. Split the sample into ~80% for training and ~20% for evaluation.\n",
    "4. Use the $k$NN algorithm and the training data to predict `price` using `year` and `mileage` for the test set for $k=3,10,25,50,100,300$. For each value of $k$, compute the mean squared error and print a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase $k$?\n",
    "5. Determine the optimal $k$ for these data.\n",
    "6. Describe what happened in the plots of predicted versus actual prices as $k$ varied, taking your answer into part 6 into account. (Hint: Use the words \"underfitting\" and \"overfitting\".)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2da6280f-9894-486a-ba2a-f45a755b4e10",
   "metadata": {},
   "source": [
    "1. The dataset has 2499 rows and 3 columns after dropping the other variables. There are no 'NA' values in price, year, or mileage so no missing data handling is required. \n",
    "\n",
    "2. This is done in the code file \"Q2.py\".\n",
    "\n",
    "3. This is done in the code file \"Q2.py\".\n",
    "\n",
    "4. Small k values produce very noisy predictions. Moderate k values produce smoother predictions and there are less error prone. Very large k values produce overly smooth predictions that ignore the local structure.\n",
    "\n",
    "5. The optimal value of k is 50 because the lowest test MSE occurs at that value.\n",
    "\n",
    "6. For small k values such as 3, the model overfits. This results in predictions following noise in the training data which leads to higher variance and poor generalization. For large k values such as 300, the model underfits. This results in predictions that are overly averaged and fail to capture meaningful relationships between year, mileage, and price. For moderate k values such as 50, the model is able to achieve a good balance between bias and variance. This produces predictions that align best with the actual prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b88334-d7ae-47ba-99c0-1be3227e8ba1",
   "metadata": {},
   "source": [
    "**Q3.** This is a case study on $k$ nearest neighbor classification, using the `animals.csv` data.\n",
    "\n",
    "The data consist of a label, `class`, taking integer values 1 to 7, the name of the species, `animal`, and 16 characteristics of the animal, including `hair`, `feathers`, `milk`, `eggs`, `airborne`, and so on. \n",
    "\n",
    "1. Load the data. For each of the seven class labels, print the values in the class and get a sense of what is included in that group. Perform some other EDA: How big are the classes? How much variation is there in each of the features/covariates? Which variables do you think will best predict which class?\n",
    "2. Split the data 50/50 into training and test/validation sets. (The smaller the data are, the more equal the split should be, in my experience: Otherwise, all of the members of one class end up in the training or test data, and the model falls apart.)\n",
    "3. Using all of the variables, build a $k$-NN classifier. Explain how you select $k$.\n",
    "4. Print a confusion table for the optimal model, comparing predicted and actual class label on the test set. How accurate it is? Can you interpret why mistakes are made across groups?\n",
    "5. Use only `milk`, `aquatic`, and `airborne` to train a new $k$-NN classifier. Print your confusion table. Mine does not predict all of the classes, only a subset of them. To see the underlying probabilities, use `model.predict_proba(X_test.values)` to predict probabilities rather than labels for your `X_test` test data for your fitted `model`. Are all of the classes represented? Explain your results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b446fad3-8b78-4a66-9101-b86720eafa2c",
   "metadata": {},
   "source": [
    "There is no 'animals.csv' file in the data directory. There is a file named 'zoo.csv' that has the relevant data so that spreadsheet was used instead.\n",
    "\n",
    "1. The dataset has 101 rows and 18 columns. For class sizes, while class 1 is the largest with 41 animals, class 5 has 4 animals and class 3 has 5 animals making them very small. Looking at the animals in each class, they roughly correspond to different biological taxonimal classifcation of groups of animals with mammals, birds, and insects.\n",
    "\n",
    "2. This is done in the code file \"Q3.py\".\n",
    "\n",
    "3. The k-NN classifier was built using the 16 feature variables which excluded the animal names. The k value was selected after trying a variety of values and odd numbers were selected in order to break ties. The k value with the best cross validation accuracy on the training set was 1. This most likely happened because the feature patterns were very distinct.  \n",
    "\n",
    "4. All features were used and the k value that was selected was 1. The test accuracy was approximately 96.1%. The reason mistakes were made across groups were due to the size of some of the classes. Two were class 3 and 5 because they had few examples and some animals share similar feature combinations. As there is too little data, a single close neighbor could possibley come from the incorrect group.  \n",
    "\n",
    "5. All classes were presented. However, since only 3 features were used to train the k-NN classifier, there was not enough information to clearly separate each class. Due to this, the model tended to assign higher probability to a smaller subset of classes. Some labels were never chosen as the most likely prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89b847",
   "metadata": {},
   "source": [
    "**Q4.** Write your own function to make a kernel density plot.\n",
    "\n",
    "- The user should pass in a Pandas series or Numpy array.\n",
    "- The default kernel should be Gaussian, but include the uniform/bump and Epanechnikov as alternatives.\n",
    "- The default bandwidth should be the Silverman plug-in, but allow the user to specify an alternative.\n",
    "- You can use Matplotlib or Seaborn's `.lineplot`, but not an existing function that creates kernel density plots.\n",
    "\n",
    "You will have to make a lot of choices and experiment with getting errors. Embrace the challenge and track your choices in the comments in your code.\n",
    "\n",
    "Use a data set from class to show that your function works, and compare it with the Seaborn `kdeplot`.\n",
    "\n",
    "We covered the Gaussian,\n",
    "$$\n",
    "k(z) = \\dfrac{1}{\\sqrt{2\\pi}}e^{-z^2/2}\n",
    "$$\n",
    "and uniform \n",
    "$$\n",
    "k(z) = \\begin{cases} \n",
    "\\frac{1}{2}, & |z| \\le 1 \\\\\n",
    "0, & |z|>1\n",
    "\\end{cases}\n",
    "$$\n",
    "kernels in class, but the Epanechnikov kernel is \n",
    "$$\n",
    "k(z) = \\begin{cases} \n",
    "\\frac{3}{4} (1-z^2), & |z| \\le 1 \\\\\n",
    "0, & |z|>1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In order to make your code run reasonably quickly, consider using the `pdist` or `cdist` functions from SciPy to make distance calculations for arrays of points. The other leading alternative is to thoughtfully use NumPy's broadcasting features. Writing `for` loops will be slow, but that's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1ad76-7b2b-4955-b128-8d8044ee948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "The code file for the above problem is in the file Q4.py."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
